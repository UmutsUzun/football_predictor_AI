# -*- coding: utf-8 -*-
"""FINAL AI11.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LdmnMEvtpjdqV2HtbLLdNJtYXxOybCgB
"""

from  google.colab import files
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
import numpy as np
from tensorflow.keras.layers import LSTM, Dropout
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

uploaded = files.upload()

pl_result_dataset = pd.read_csv('results.csv', encoding='latin1')
pl_result_dataset.head() # See to situation of dataset

print(pl_result_dataset.info())

pl_result_dataset.shape #Check the number of data

#CLEANING SECTION
pl_result_dataset.isna().sum() #Check the missing datas.

# Filling in missing data
pl_result_dataset.fillna({
    'HTHG': pl_result_dataset['HTHG'].mean(),
    'HTAG': pl_result_dataset['HTAG'].mean(),
    'HTR': pl_result_dataset['HTR'].mode()[0],
    'HS': pl_result_dataset['HS'].mean(),
    'AS': pl_result_dataset['AS'].mean(),
    'HST': pl_result_dataset['HST'].mean(),
    'AST': pl_result_dataset['AST'].mean(),
    'HC': pl_result_dataset['HC'].mean(),
    'AC': pl_result_dataset['AC'].mean(),
    'HF': pl_result_dataset['HF'].mean(),
    'AF': pl_result_dataset['AF'].mean(),
    'HY': pl_result_dataset['HY'].mean(),
    'AY': pl_result_dataset['AY'].mean(),
    'HR': pl_result_dataset['HR'].mean(),
    'AR': pl_result_dataset['AR'].mean()
}, inplace=True)
pl_result_dataset.head() #See to result after this codes.

# For see to all columns
pd.set_option('display.max_columns', None)
pl_result_dataset.describe().round()

#I am dropping the referee column because I cannot establish any function or relationship and the change over the years is very rapid and its type is string
pl_result_dataset = pl_result_dataset.drop(['Referee'], axis=1)

pl_result_dataset.isna().sum() #See to after this process missing data situation.

#I'm dropping the Season column because Datetime column gives better time than that anyway.
pl_result_dataset = pl_result_dataset.drop(['Season'], axis=1)

#CONVERTING DATA INTO NUMERICAL VALUES SECTION
# Convert dates
pl_result_dataset['DateTime'] = pd.to_datetime(pl_result_dataset['DateTime'])

# Create a list of unique teams
unique_teams = pd.concat([pl_result_dataset['HomeTeam'], pl_result_dataset['AwayTeam']]).unique()

# Assign a unique ID to teams
team_ids = {team: idx for idx, team in enumerate(unique_teams, start=1)}

# Add these IDs to the original dataset
pl_result_dataset['HomeTeamID'] = pl_result_dataset['HomeTeam'].map(team_ids)
pl_result_dataset['AwayTeamID'] = pl_result_dataset['AwayTeam'].map(team_ids)
pl_result_dataset.head()

#Since I gave teams special IDs like HomeTeamID and AwayTeamID, I no longer need the hometeam and awayteam columns, so I'm removing them.
pl_result_dataset = pl_result_dataset.drop(columns=['HomeTeam', 'AwayTeam'])
pl_result_dataset.head()

#Convert Full time result and Half time result columns H,D,A to numbers  1,0,2
pl_result_dataset['FTR'] = pl_result_dataset['FTR'].map({'H': 1, 'D': 0, 'A': 2})
pl_result_dataset['HTR'] = pl_result_dataset['HTR'].map({'H': 1, 'D': 0, 'A': 2})

pl_result_dataset.head()

# #PREPROCESSING SECTION
# #I will strengthen my correlation by establishing relationships between features.
# Function that calculates teams' last match performances
def calculate_team_performance(df):
    df = df.sort_values(by='DateTime')
    df['HomeTeamRecentGames'] = df.groupby('HomeTeamID')['FTHG'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())
    df['AwayTeamRecentGames'] = df.groupby('AwayTeamID')['FTAG'].transform(lambda x: x.rolling(window=5, min_periods=1).mean())
    return df

pl_result_dataset = calculate_team_performance(pl_result_dataset)

# Function to calculate goal per shot ratio
def add_shot_to_goal_ratio(df):
    df['HomeShotsToGoalsRatio'] = df['HS'] / (df['FTHG'] + 1)
    df['AwayShotsToGoalsRatio'] = df['AS'] / (df['FTAG'] + 1)
    return df

pl_result_dataset = add_shot_to_goal_ratio(pl_result_dataset)

# Function to calculate goals per shot ratio of each team
def calculate_team_goal_ratios(df):
    home_team_ratios = df.groupby('HomeTeamID').apply(lambda x: x['FTHG'].sum() / (x['HS'].sum() + 1)).reset_index(name='HomeTeamGoalRatio')
    away_team_ratios = df.groupby('AwayTeamID').apply(lambda x: x['FTAG'].sum() / (x['AS'].sum() + 1)).reset_index(name='AwayTeamGoalRatio')

    df = df.merge(home_team_ratios, on='HomeTeamID', how='left')
    df = df.merge(away_team_ratios, on='AwayTeamID', how='left')
    return df

pl_result_dataset = calculate_team_goal_ratios(pl_result_dataset)

pl_result_dataset.isna().sum()

# Check the distribution of classes (Check the balance)
class_distribution = pl_result_dataset['FTR'].value_counts()

# Print the class distribution
print("Class Distribution:\n", class_distribution)

# Create Graph
plt.figure(figsize=(8, 6))
sns.barplot(x=class_distribution.index, y=class_distribution.values, palette='viridis')
plt.title('Class Distribution (Home Win, Draw, Away Win)')
plt.xlabel('Outcome')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(20, 8))
sns.heatmap(pl_result_dataset.corr(), annot=True, cmap='coolwarm')
plt.title('Heatmap of Numerical Features')
plt.show()

#DATA SPLIT SECTION
#The reason I did not add some columns is that the new columns formed by the functional connections between the two columns I already established were created by using the columns I did not already add.

# Specify features and target variable
features = [ 'HTHG', 'HTAG', 'HTR','HST','AST','HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR', 'HomeTeamID', 'AwayTeamID', 'HomeTeamRecentGames', 'AwayTeamRecentGames', 'HomeShotsToGoalsRatio', 'AwayShotsToGoalsRatio', 'HomeTeamGoalRatio', 'AwayTeamGoalRatio']
target = 'FTR'

# Spliting features and target variables
X = pl_result_dataset[features].values
y = pl_result_dataset[target].values

# Spliting data into training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

#Scaling section

scaler = MinMaxScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=features)

# Plotting graphs
X_train_scaled_df.hist(bins=20, figsize=(15, 12), color='red', edgecolor='yellow')
plt.suptitle('Feature Distributions After MinMax Scaling', fontsize=16)
plt.show()

# Reshaping the data for LSTM model
X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))

# Building the LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(100, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True))
lstm_model.add(Dropout(0.2))
lstm_model.add(LSTM(100, return_sequences=False))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(50, activation='relu'))
lstm_model.add(Dense(3, activation='softmax'))  

# Compile
lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Training 
lstm_model.fit(X_train_lstm, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Make prediction 
y_pred_lstm = lstm_model.predict(X_test_lstm)
y_pred_lstm = np.argmax(y_pred_lstm, axis=1)

# Calculate performance 
lstm_accuracy = accuracy_score(y_test, y_pred_lstm)
lstm_f1 = f1_score(y_test, y_pred_lstm, average='weighted')
lstm_precision = precision_score(y_test, y_pred_lstm, average='weighted')
lstm_recall = recall_score(y_test, y_pred_lstm, average='weighted')

# Classification Report for LSTM
lstm_report = classification_report(y_test, y_pred_lstm)
print("Classification Report for LSTM:\n", lstm_report)

# Generating Confusion Matrix for LSTM
lstm_cm = confusion_matrix(y_test, y_pred_lstm)
lstm_disp = ConfusionMatrixDisplay(confusion_matrix=lstm_cm, display_labels=['Draw', 'Home Win', 'Away Win'])
lstm_disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for LSTM')
plt.show()

# Implementation of the RandomForest model

rf_model = RandomForestClassifier(random_state=0)
rf_model.fit(X_train_scaled, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test_scaled)

# Calculate the Accuracy
accuracy = accuracy_score(y_test, y_pred_rf)
print("RandomForest Modelling Accuracy:", accuracy)

# Create Classification Report
report = classification_report(y_test, y_pred_rf)
print("Classification Report:\n", report)

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred_rf)

# Display Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Draw', 'Home Win', 'Away Win'])
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for RandomForest')
plt.show()

#RandomForest Model with Hyper Parameter Tuning

# Define the parameter grid
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

rf_model_tuned = RandomForestClassifier(random_state=0)

grid_search = GridSearchCV(estimator=rf_model_tuned, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit 
grid_search.fit(X_train_scaled, y_train)

# for best parameters
best_params = grid_search.best_params_
print("Best Parameters:\n", best_params)

# best estimator for predictions
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test_scaled)

# Calculate accuracy 
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("RandomForest Modelling Accuracy with Best Parameters:", accuracy)
print("Classification Report with Best Parameters:\n", report)

# Implementation of the Feedforward Neural Networks (FNN)

# Convert labels 
y_train_cat = to_categorical(y_train)
y_test_cat = to_categorical(y_test)

# Create the model
fnn_model = Sequential()
fnn_model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))
fnn_model.add(Dense(64, activation='relu'))
fnn_model.add(Dense(32, activation='relu'))
fnn_model.add(Dense(3, activation='softmax'))

# Compile the model
fnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = fnn_model.fit(X_train_scaled, y_train_cat, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# Prediction
y_pred_prob = fnn_model.predict(X_test_scaled)
y_pred_fnn = y_pred_prob.argmax(axis=1)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_fnn)
report = classification_report(y_test, y_pred_fnn)

# Calculate and print model accuracy
print("Feedforward Neural Networks Accuracy:", accuracy)
print("Classification Report:\n", report)

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred_fnn)

# Display the Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Draw', 'Home Win', 'Away Win'])
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for Feedforward Neural Networks')
plt.show()

#Implementation of the SVM model(Support Vector Machine )
#Create Model
svm_model = SVC(kernel='linear', random_state=0)

#Train to model
svm_model.fit(X_train_scaled, y_train)

#Do prediction
y_pred_svm = svm_model.predict(X_test_scaled)

# Calculate the Accuracy
accuracy = accuracy_score(y_test, y_pred_svm)
print(f"SVM Modelling Accuracy: {accuracy}")

#Create Report
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# Generate Confusion Matrix
cm = confusion_matrix(y_test, y_pred_svm)

# Display the Confusion Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Draw', 'Home Win', 'Away Win'])
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for SVM')
plt.show()

#Implementation of the hyperparameter version of SVM model(Support Vector Machine )
#Parameters
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Initialize the SVM model
svm_model_tuned = SVC(random_state=0)

# Initialize parameter grid
grid_search = GridSearchCV(estimator=svm_model_tuned, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit 
grid_search.fit(X_train_scaled, y_train)

# Find best parameters and best estimator
best_params = grid_search.best_params_
print("Best Parameters:\n", best_params)
best_svm = grid_search.best_estimator_

# Predict best estimator
y_pred_svm_tuned = best_svm.predict(X_test_scaled)

# Calculate the Accuracy
accuracy = accuracy_score(y_test, y_pred_svm_tuned)
print(f"SVM Modelling Accuracy with Best Parameters: {accuracy}")

# Create Classification Report
print("Classification Report:\n", classification_report(y_test, y_pred_svm_tuned))

# Generate  Matrix
cm = confusion_matrix(y_test, y_pred_svm_tuned)

# Display Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Draw', 'Home Win', 'Away Win'])
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for SVM')
plt.show()

# Implementation of the Gradient Boosting Classifier
gbc_model = GradientBoostingClassifier(random_state=0)

gbc_model.fit(X_train_scaled, y_train)
y_pred_gbc = gbc_model.predict(X_test_scaled)

# Calculate the model accuracy
accuracy = accuracy_score(y_test, y_pred_gbc)
print(f"Gradient Boosting Classifier Accuracy: {accuracy}")

# Creating report
print("Classification Report:")
print(classification_report(y_test, y_pred_gbc))

# generate  Matrix
cm = confusion_matrix(y_test, y_pred_gbc)

# Display  Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Draw', 'Home Win', 'Away Win'])
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for Gradient Boosting Classifier')
plt.show()

#Hyperparameter for Gradient Boosting Modelling
# Paramateres
param_grid = {
    'n_estimators': [100, 150],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 4],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [2, 4]
}

gbm_model_tuned = GradientBoostingClassifier(random_state=0)

# Grid settings
grid_search = GridSearchCV(estimator=gbm_model_tuned, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

# Fit 
grid_search.fit(X_train_scaled, y_train)

# Get the best parameters
best_params = grid_search.best_params_
print("Best Parameters:\n", best_params)

# Use the best estimator to make predictions
best_gbm = grid_search.best_estimator_
y_pred_gbm_tuned = best_gbm.predict(X_test_scaled)

# Calculate the model accuracy
accuracy = accuracy_score(y_test, y_pred_gbm_tuned)
print(f"Gradient Boosting Classifier Accuracy with Best Parameters: {accuracy}")

# Creating report
print("Classification Report:")
print(classification_report(y_test, y_pred_gbm_tuned))

# GenerateMatrix
cm = confusion_matrix(y_test, y_pred_gbm_tuned)

# Display Matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Draw','Home Win','Away Win'])
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title('Confusion Matrix for Gradient Boosting Classifier (Optimized)')
plt.show()

# Random Forest Metrics
rf_accuracy = accuracy_score(y_test, y_pred_rf)
rf_f1 = f1_score(y_test, y_pred_rf, average='weighted')
rf_precision = precision_score(y_test, y_pred_rf, average='weighted')
rf_recall = recall_score(y_test, y_pred_rf, average='weighted')

# Feedforward Neural Network Metrics
ann_accuracy = accuracy_score(y_test, y_pred_fnn)
ann_f1 = f1_score(y_test, y_pred_fnn, average='weighted')
ann_precision = precision_score(y_test, y_pred_fnn, average='weighted')
ann_recall = recall_score(y_test, y_pred_fnn, average='weighted')

# SVM (Hyperparameter Tuning) Metrics
svm_accuracy = accuracy_score(y_test, y_pred_svm_tuned)
svm_f1 = f1_score(y_test, y_pred_svm_tuned, average='weighted')
svm_precision = precision_score(y_test, y_pred_svm_tuned, average='weighted')
svm_recall = recall_score(y_test, y_pred_svm_tuned, average='weighted')

# Gradient Boosting (Hyperparameter Tuning) Metrics
gbm_accuracy = accuracy_score(y_test, y_pred_gbm_tuned)
gbm_f1 = f1_score(y_test, y_pred_gbm_tuned, average='weighted')
gbm_precision = precision_score(y_test, y_pred_gbm_tuned, average='weighted')
gbm_recall = recall_score(y_test, y_pred_gbm_tuned, average='weighted')

# LSTM Model Metrics
lstm_accuracy = accuracy_score(y_test, y_pred_lstm)
lstm_f1 = f1_score(y_test, y_pred_lstm, average='weighted')
lstm_precision = precision_score(y_test, y_pred_lstm, average='weighted')
lstm_recall = recall_score(y_test, y_pred_lstm, average='weighted')

# Prepare data 
model_metrics = {
    'Model': ['Random Forest', 'Feedforward Neural Network', 'SVM (Hyperparameter Tuning)', 'Gradient Boosting (Hyperparameter Tuning)', 'LSTM'],
    'Accuracy': [rf_accuracy, ann_accuracy, svm_accuracy, gbm_accuracy, lstm_accuracy],
    'F1 Score': [rf_f1, ann_f1, svm_f1, gbm_f1, lstm_f1],
    'Precision': [rf_precision, ann_precision, svm_precision, gbm_precision, lstm_precision],
    'Recall': [rf_recall, ann_recall, svm_recall, gbm_recall, lstm_recall]
}

# Convert to DataFrame
metrics_df = pd.DataFrame(model_metrics)

# Plotting the comparison
plt.figure(figsize=(14, 8))

# Define colors
colors = ['red', 'black', 'blue', 'green', 'purple']  

# Create the bar plot
ax = metrics_df.set_index('Model').plot(kind='bar', figsize=(14, 8), color=colors, edgecolor='black')

# Add plot details
plt.title('Comparison of Model Performance Metrics')
plt.ylabel('Score')
plt.ylim(0.7,1) 
plt.xticks(rotation=45, ha='right')
plt.legend(loc='upper right', bbox_to_anchor=(1.15, 1))  

# Display the plot
plt.tight_layout()
plt.show()

# Function to calculate model performances
def calculate_metrics(y_true, y_pred):
    return {
        'Accuracy': accuracy_score(y_true, y_pred),
        'F1 Score': f1_score(y_true, y_pred, average='weighted'),
        'Precision': precision_score(y_true, y_pred, average='weighted'),
        'Recall': recall_score(y_true, y_pred, average='weighted')
    }

model_metrics = {}

# Random Forest Metrics
model_metrics['Random Forest'] = calculate_metrics(y_test, y_pred_rf)

# Feedforward Neural Network (FNN) Metrics
model_metrics['Feedforward Neural Network'] = calculate_metrics(y_test, y_pred_fnn)

# SVM (Hyperparameter Tuning) Metrics
model_metrics['SVM (Hyperparameter Tuning)'] = calculate_metrics(y_test, y_pred_svm_tuned)

# Gradient Boosting (Hyperparameter Tuning) Metrics
model_metrics['Gradient Boosting (Hyperparameter Tuning)'] = calculate_metrics(y_test, y_pred_gbm_tuned)

# LSTM Model Metrics
model_metrics['LSTM'] = calculate_metrics(y_test, y_pred_lstm)

# Convert metrics to DataFrame
metrics_df = pd.DataFrame(model_metrics).T
metrics_df.reset_index(inplace=True)
metrics_df.columns = ['Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall']

# Creating graphs for all metrics
metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall']
for metric in metrics:
    plt.figure(figsize=(10, 6))
    plt.bar(metrics_df['Model'], metrics_df[metric], color='red', edgecolor='black')
    plt.title(f'Comparison of Model {metric}')
    plt.ylabel(metric)
    plt.ylim(0.7, 1)  
    plt.xticks(rotation=45)
    plt.show()